[[aws-serverless-troubleshooting]]
= Troubleshooting and error handling

++++
<titleabbrev>Troubleshooting</titleabbrev>
++++

[[aws-serverless-troubleshooting-1.6.0-id-changes]]
== New Event ID format starting from v1.6.0

Version 1.6.0 introduces a new event ID format, to prevent duplicate ID errors when a high volume of events is ingested to {es}. This new format combines a timestamp and data specific to the AWS resource involved, both taken from the AWS Lambda event that Elastic Serverless Forwarder receives.
The timestamp is used as prefix for the ID, as identifiers that gradually increase over time based on sorting order generally result in better indexing performance in {es} than completely random identifiers. More information about this are available on https://www.elastic.co/blog/efficient-duplicate-prevention-for-event-based-data-in-elasticsearch[this blog post]

**Please be aware that, if old events that were already published to {es} using older versions of Elastic Serverless Forwarder are ingested again, they will be treated as new events and published to {es} again.**

== Troubleshooting deployment errors
You can view the status of deployment actions and get additional information on events, including why a particular event fails e.g. misconfiguration details.

. On the Applications page for **serverlessrepo-elastic-serverless-forwarder**, click **Deployments**.
. You can view the **Deployment history** here and refresh the page for updates as the application deploys. It should take around 5 minutes to deploy &mdash; if the deployment fails for any reason, the create events will be rolled back and you will be able to see an explanation for which event failed.

NOTE: For example, if you don't increase the visibility timeout for an SQS queue as described in <<aws-serverless-forwarder-inputs-s3>>, you will see a `CREATE_FAILED`**Status** for the event, and the **Status reason** provides additional detail.

[[aws-serverless-troubleshooting-vpc-prerequisites]]
== Prerequisites when attached to a VPC
If the Elastic Serverless Forwarder is attached to a VPC, you need to https://docs.aws.amazon.com/vpc/latest/privatelink/create-interface-endpoint.html[create VPC Endpoints] for S3 and SQS, and for *every* service you define as an input for the forwarder. S3 and SQS VPC Endpoints are always required for reading the `config.yaml` uploaded to S3 and managing the _Continuing queue_ and the _Replay queue_, regardless of the <<aws-serverless-forwarder-inputs>> used.

== Preventing unexpected costs
It is important to monitor the Elastic Serverless Forwarder Lambda function for timeouts to prevent unexpected costs. You can use the https://docs.elastic.co/en/integrations/aws/lambda[AWS Lambda integration] for this. If the timeouts are constant, you should throttle the Lambda function to stop its execution before proceeding with any troubleshooting steps. In most cases, constant timeouts will cause the records and messages from the event triggers to go back to their sources and trigger the function again, which will cause further timeouts and force a loop that will incure unexpected high costs.

// is it clear how you would throttle the Lambda function? should we detail and number these steps?

=== Increase debug information
To help with debugging, you can increase the amount of logging detail by adding an environment variable as follows:

. Select the serverless forwarder **application** from **Lambda > Functions**
. Click **Configuration** and select **Environment Variables** and choose **Edit**
. Click **Add environment variable** and enter `LOG_LEVEL` as **Key** and `DEBUG` as **Value** and click **Save**

// confirm where this is visible - only in CloudWatch or also within ES messages?

== Error handling

There are two kind of errors that can occur during execution of the forwarder:

. Errors _before_ the ingestion phase begins
. Errors _during_ the ingestion phase

=== Errors before ingestion
For errors that occur before ingestion begins (1), the function will return a failure. These errors are mostly due to misconfiguration, incorrect permissions for AWS resources, etc. Most importantly, when an error occurs at this stage we don’t have any status for events that are ingested, so there’s no requirement to keep data, and the function can fail safely. In the case of SQS messages and Kinesis data stream records, both will go back into the queue and trigger the function again with the same payload.

=== Errors during ingestion
For errors that occur during ingestion (2), the situation is different. We do have a status for *N* failed events out of *X* total events; if we fail the whole function then all *X* events will be processed again. While the *N* failed ones could potentially succeed, the remaining *X-N* will definitely fail, because the data streams are append-only (the function would attempt to recreate already ingested documents using the same document ID).

So the forwarder won't return a failure for errors during ingestion; instead, the payload of the event that failed to be ingested will be sent to a replay SQS queue, which is automatically set up during the deployment. The replay SQS queue is not set as an Event Source Mapping for the function by default, which means you can investigate and consume (or not) the message as preferred.

You can temporarily set the replay SQS queue as an Event Source Mapping for the forwarder, which means messages in the queue will be consumed by the function and ingestion retried for transient failures. If the failure persists, the affected log entry will be moved to a DLQ after three retries.

Every other error that occurs during the execution of the forwarder is silently ignored, and reported to the APM server if instrumentation is enabled.

=== Execution timeout
There is a grace period of 2 minutes before the timeout of the Lambda function where no more ingestion will occur. Instead, during this grace period the forwarder will collect and handle any unprocessed payloads in the batch of the input used as trigger.

For CloudWatch Logs event, Kinesis data stream, S3 SQS Event Notifications and direct SQS message payload inputs, the unprocessed batch will be sent to the SQS continuing queue.
